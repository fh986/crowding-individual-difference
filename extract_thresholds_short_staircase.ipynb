{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "266e29fa",
   "metadata": {},
   "source": [
    "This notebook extracts thresholds predicted by quest after every trial. \n",
    "It saves the results in a long form (each row is a threshold), named \"tidy_both_sessions_per_trial_log.csv\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1d44a580-e398-466d-a9b2-17e3abc9db91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install seaborn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "13f644fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import seaborn as sns\n",
    "import scipy.stats as stats\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ddaa1d94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of total sessions:  360\n",
      "Number of subjects who completed the second session:  171\n"
     ]
    }
   ],
   "source": [
    "# Set up files\n",
    "dir_firstSess = 'data/corr_firstSess'\n",
    "files_firstSess = [f for f in os.listdir(dir_firstSess) if f.endswith('.csv')]\n",
    "\n",
    "dir_secondSess = 'data/corr_secondSess'\n",
    "files_secondSess = [f for f in os.listdir(dir_secondSess) if f.endswith('.csv')]\n",
    "\n",
    "numTotalSessions = len(files_firstSess) + len(files_secondSess)\n",
    "\n",
    "print('Number of total sessions: ', numTotalSessions)\n",
    "print('Number of subjects who completed the second session: ', len(files_secondSess))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2f1c7cbc-056d-4068-b4bf-eb9986afff51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('------first session------')\n",
    "# for f in files_firstSess:\n",
    "#     print(f)\n",
    "    \n",
    "# print('------second session------')\n",
    "# for f in files_secondSess:\n",
    "#     print(f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "210b9b01",
   "metadata": {},
   "source": [
    "## Acquire thresholds for all tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5628276",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8b006083",
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkIfComplete(mainOutput):\n",
    "    \n",
    "    complete_val = mainOutput['experimentCompleteBool'].dropna().iloc[0]\n",
    "    complete_bool = str(complete_val) == 'True'\n",
    "    if not complete_bool:\n",
    "        prolificID = mainOutput['ProlificParticipantID'].dropna().iloc[0]\n",
    "    assert complete_bool, 'Fatal: experiment not complete!'                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "96846bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_condition(condition_str):\n",
    "    \"\"\"\n",
    "    Parse a condition string of the form <task>_<meridian>_block<repeat>\n",
    "    Example: 'acuity_R8_block1' -> ('acuity', 'R8', 1)\n",
    "    \"\"\"\n",
    "    match = re.match(r\"^(.*?)_(.*?)_block(\\d+)$\", condition_str)\n",
    "    if match:\n",
    "        task = match.group(1)\n",
    "        meridian = match.group(2)\n",
    "        repeat = int(match.group(3))\n",
    "        return task, meridian, repeat\n",
    "    else:\n",
    "        raise ValueError(f\"String '{condition_str}' is not in the expected format.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "31cb1596",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getThresholds(mydir, files, condition_names, linear_scale_bool = False, staircase_length = 23, convert_to_wpm_bool = False): # num_trials_per_staircase=35, exclude_trial_count_bool=True, exclude_questSD=True, \n",
    "\n",
    "    all_data = []\n",
    "\n",
    "    numSess = len(files)\n",
    "    numThresholdsCat = len(condition_names)\n",
    "\n",
    "    for sess in range(numSess):\n",
    "\n",
    "        # Read the CSV file\n",
    "        file_path = os.path.join(mydir, files[sess])\n",
    "        mainOutput = pd.read_csv(file_path)\n",
    "        checkIfComplete(mainOutput)\n",
    "\n",
    "        for cat in range(numThresholdsCat):\n",
    "\n",
    "            cond_threshold = {}\n",
    "            prolificID = mainOutput['ProlificParticipantID'].dropna().iloc[0]\n",
    "            assert prolificID, 'Fatal: no prolificID'\n",
    "\n",
    "            condition_threshold = 0\n",
    "            condition_name = condition_names[cat]\n",
    "            taskName, meridian, repeat = parse_condition(condition_name)\n",
    "\n",
    "            condition_data = mainOutput[mainOutput['conditionName'] == condition_name]\n",
    "\n",
    "            condition_data_filtered = condition_data[condition_data['trialGivenToQuest'] == True]\n",
    "            condition_staircase_full = condition_data_filtered['questMeanBeforeThisTrialResponse'].dropna()\n",
    "\n",
    "            if len(condition_staircase_full) > staircase_length - 1:\n",
    "                condition_logThreshold = condition_staircase_full.iloc[staircase_length - 1]\n",
    "            elif len(condition_staircase_full) == 0:\n",
    "                print('Warning: No trials sent to QUEST (file name: {}, condition {})'.format(files[sess], condition_name))\n",
    "                condition_logThreshold = np.nan\n",
    "            else:\n",
    "                condition_logThreshold = condition_staircase_full.iloc[-1]\n",
    "            assert np.isscalar(condition_logThreshold), \"Threshold extraction did not return a single value\"\n",
    "\n",
    "            if convert_to_wpm_bool:\n",
    "                condition_logThreshold = np.log10(60) - condition_logThreshold\n",
    "\n",
    "            if linear_scale_bool:\n",
    "            # Convert to linear scale \n",
    "                condition_threshold = 10 ** condition_logThreshold\n",
    "            else:\n",
    "                condition_threshold = condition_logThreshold\n",
    "\n",
    "\n",
    "            # number of trials\n",
    "            trial_sent = condition_data['trialGivenToQuest']\n",
    "            num_trial_sent = sum(str(this_trial) == 'True' for this_trial in trial_sent)\n",
    "            # questSD\n",
    "            questSD = condition_data['questSDAtEndOfTrialsLoop'].dropna().iloc[0]\n",
    "\n",
    "            assert condition_threshold != 0, 'Fatal: Threshold not assigned'\n",
    "\n",
    "            cond_threshold['prolificID'] = prolificID\n",
    "            cond_threshold['conditionName'] = condition_name\n",
    "            cond_threshold['taskName'] = taskName\n",
    "            cond_threshold['meridian'] = meridian\n",
    "            cond_threshold['repeat'] = repeat\n",
    "            cond_threshold['threshold'] = condition_threshold\n",
    "            cond_threshold['numTrialsSent'] = num_trial_sent\n",
    "            cond_threshold['questSD'] = questSD\n",
    "            cond_threshold['readingCQAccuracy'] = np.nan\n",
    "\n",
    "            all_data.append(cond_threshold)\n",
    "\n",
    "            # all_data.append(subj_thresholds)\n",
    "\n",
    "    all_data_df = pd.DataFrame(all_data)\n",
    "\n",
    "    return all_data_df\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "31027c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getOrdReadingSpeed(mydir, files, condition_dict, sessionName, log_bool = True):\n",
    "    '''\n",
    "    accuracy_criterion_percent: the reading speed will be marked as np.nan if the accuracy for the comprehension question\n",
    "                                is lower than this percentage\n",
    "    wpm_criteiron: the reading speed will be marked as np.nan if it is higher than this percentage\n",
    "    '''\n",
    "    \n",
    "    condition_names = list(condition_dict.keys())\n",
    "    \n",
    "    all_data = []\n",
    "    \n",
    "    numSess = len(files)\n",
    "    numThresholdsCat = len(condition_names)\n",
    "    \n",
    "    for sess in range(numSess):\n",
    "        \n",
    "        # Read the CSV file\n",
    "        file_path = os.path.join(mydir, files[sess])\n",
    "        mainOutput = pd.read_csv(file_path)\n",
    "        checkIfComplete(mainOutput)    \n",
    "        \n",
    "        for cat in range(numThresholdsCat):\n",
    "\n",
    "            cond_wpm = {}\n",
    "                \n",
    "            prolificID = mainOutput['ProlificParticipantID'].dropna().iloc[0]\n",
    "            \n",
    "            wordsPerMin = 0\n",
    "            condition_name = condition_names[cat]\n",
    "            taskName, meridian, repeat = parse_condition(condition_name)\n",
    "            repeat = sessionName # for reading, the repeats in one session are assigned to the same repeat\n",
    "\n",
    "            # Check if the participant answered 3 or more questions correctly\n",
    "            question_labels = condition_dict[condition_name]\n",
    "            num_questions = len(question_labels)\n",
    "            question_correct_bool = np.full(num_questions,np.nan)\n",
    "            for qq in range(num_questions):\n",
    "\n",
    "                qq_data = mainOutput[mainOutput['questionAndAnswerNickname'] == question_labels[qq]]\n",
    "                \n",
    "                question_correct_bool[qq] = (qq_data['questionAndAnswerCorrectAnswer'].item() == qq_data['questionAndAnswerResponse'].item())\n",
    "                \n",
    "            percent_correct = sum(question_correct_bool) / num_questions * 100 \n",
    "            \n",
    "            # calculate reading speed\n",
    "            speed_data = mainOutput[mainOutput['conditionName'] == condition_name]\n",
    "            numWords = speed_data['readingPageWords'].dropna()\n",
    "            reading_time = speed_data['readingPageDurationOnsetToOffsetSec'].dropna()\n",
    "\n",
    "            numWords_include = numWords[1:len(numWords)-1]  # exclude first and last page\n",
    "            reading_time_include = reading_time[1:len(reading_time)-1]\n",
    "\n",
    "            numWords_sum = numWords_include.sum()\n",
    "            reading_time_sum = reading_time_include.sum()\n",
    "\n",
    "            readingSpeed = numWords_sum / (reading_time_sum / 60)\n",
    "\n",
    "            assert readingSpeed != 0, 'Fatal: Threshold not assigned'\n",
    "\n",
    "            if log_bool:\n",
    "                readingSpeed = np.log10(readingSpeed)\n",
    "\n",
    "            cond_wpm['prolificID'] = prolificID\n",
    "            cond_wpm['conditionName'] = condition_name\n",
    "            cond_wpm['taskName'] = taskName\n",
    "            cond_wpm['meridian'] = meridian\n",
    "            cond_wpm['repeat'] = repeat\n",
    "            cond_wpm['threshold'] = readingSpeed\n",
    "            cond_wpm['numTrialsSent'] = np.nan\n",
    "            cond_wpm['questSD'] = np.nan\n",
    "            cond_wpm['readingCQAccuracy'] = percent_correct\n",
    "\n",
    "            all_data.append(cond_wpm)\n",
    "        \n",
    "        all_data_df = pd.DataFrame(all_data)\n",
    "        \n",
    "    return all_data_df\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "35d9aaca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_trialwise_thresholds(\n",
    "    mydir, files, condition_names,\n",
    "    value_col=\"questMeanBeforeThisTrialResponse\",\n",
    "    linear_scale_bool=False,\n",
    "    convert_to_wpm_bool=False\n",
    "):\n",
    "    \"\"\"\n",
    "    Return one row per QUEST trial with the running threshold value\n",
    "    taken directly from `value_col` (default: 'questMeanBeforeThisTrialResponse').\n",
    "\n",
    "    Columns returned:\n",
    "      prolificID, file, conditionName, taskName, meridian, repeat,\n",
    "      trial_index (1-based within staircase), threshold_value,\n",
    "      numTrialsSent_total (per condition, optional), questSD_end (optional)\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "\n",
    "    for fname in files:\n",
    "        file_path = os.path.join(mydir, fname)\n",
    "        main = pd.read_csv(file_path)\n",
    "        checkIfComplete(main)\n",
    "\n",
    "        prolificID = (main['ProlificParticipantID'].dropna().iloc[0]\n",
    "                      if 'ProlificParticipantID' in main else np.nan)\n",
    "        assert prolificID, f'Fatal: no prolific ID, session name: {fname}'\n",
    "\n",
    "        for condition_name in condition_names:\n",
    "            # subset to condition\n",
    "            cond = main[main['conditionName'] == condition_name].copy()\n",
    "            assert not cond.empty,f'Fatal: empty condition, session name: {fname}'\n",
    "\n",
    "            # only trials sent to QUEST\n",
    "            if 'trialGivenToQuest' not in cond.columns:\n",
    "                # nothing to extract for non-QUEST tasks\n",
    "                print(f'Warning: no column named trialGivenToQuest, session name: {fname}')\n",
    "\n",
    "            cond = cond[cond['trialGivenToQuest'] == True].copy()\n",
    "            if cond.empty:\n",
    "                print(f'Fatal: no trials were sent to quest, session name: {fname}; condition: {condition_name}')\n",
    "\n",
    "\n",
    "            # must have the value column\n",
    "            assert value_col in cond.columns, f'Fatal: no value column {value_col}, session name: {fname}'\n",
    "\n",
    "            # parse condition into parts (your helper)\n",
    "            taskName, meridian, repeat = parse_condition(condition_name)\n",
    "\n",
    "            # numeric series; if set errors to 'coerse', will keep NaNs to preserve alignment if any\n",
    "            vals = pd.to_numeric(cond[value_col], errors='raise')\n",
    "\n",
    "            # optional unit conversions (apply in log domain first)\n",
    "            if convert_to_wpm_bool:\n",
    "                # log10(sec/word) -> log10(words/min)\n",
    "                vals = np.log10(60) - vals\n",
    "\n",
    "            if linear_scale_bool:\n",
    "                vals = 10 ** vals\n",
    "\n",
    "            # optional per-condition metadata\n",
    "            numTrialsSent_total = int((main.loc[main['conditionName'] == condition_name, 'trialGivenToQuest'] == True).sum())\n",
    "            # questSD_end = (pd.to_numeric(cond.get('questSDAtEndOfTrialsLoop', pd.Series(dtype=float)), errors='coerce')\n",
    "            #                .dropna()\n",
    "            #                .iloc[0] if 'questSDAtEndOfTrialsLoop' in cond and cond['questSDAtEndOfTrialsLoop'].notna().any() else np.nan)\n",
    "\n",
    "            # emit rows\n",
    "            for t in range(len(vals)):\n",
    "                rows.append({\n",
    "                    'prolificID': prolificID,\n",
    "                    'conditionName': condition_name,\n",
    "                    'taskName': taskName,\n",
    "                    'meridian': meridian,\n",
    "                    'repeat': repeat,\n",
    "                    'trial_index': t + 1,            # 1-based\n",
    "                    'threshold': vals.iloc[t],  # taken directly from value_col\n",
    "                    'numTrialsSent': numTrialsSent_total,\n",
    "                    'readingCQAccuracy': np.nan\n",
    "                    # 'questSD_end': questSD_end,\n",
    "                })\n",
    "\n",
    "    return pd.DataFrame(rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7eb47d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getOrdReadingSpeed_pagewise(mydir, files, condition_dict, sessionName, log_bool = True):\n",
    "    '''\n",
    "    accuracy_criterion_percent: the reading speed will be marked as np.nan if the accuracy for the comprehension question\n",
    "                                is lower than this percentage\n",
    "    wpm_criteiron: the reading speed will be marked as np.nan if it is higher than this percentage\n",
    "    '''\n",
    "    \n",
    "    condition_names = list(condition_dict.keys())\n",
    "    \n",
    "    all_data = []\n",
    "    \n",
    "    numSess = len(files)\n",
    "    numThresholdsCat = len(condition_names)\n",
    "    \n",
    "    for sess in range(numSess):\n",
    "        \n",
    "        # Read the CSV file\n",
    "        file_path = os.path.join(mydir, files[sess])\n",
    "        mainOutput = pd.read_csv(file_path)\n",
    "        checkIfComplete(mainOutput)    \n",
    "        \n",
    "        for cat in range(numThresholdsCat):\n",
    "\n",
    "            cond_wpm = {}\n",
    "                \n",
    "            prolificID = mainOutput['ProlificParticipantID'].dropna().iloc[0]\n",
    "            \n",
    "            wordsPerMin = 0\n",
    "            condition_name = condition_names[cat]\n",
    "            taskName, meridian, repeat = parse_condition(condition_name)\n",
    "            repeat = sessionName # for reading, the repeats in one session are assigned to the same repeat\n",
    "\n",
    "            # Check if the participant answered 3 or more questions correctly\n",
    "            question_labels = condition_dict[condition_name]\n",
    "            num_questions = len(question_labels)\n",
    "            question_correct_bool = np.full(num_questions,np.nan)\n",
    "            for qq in range(num_questions):\n",
    "\n",
    "                qq_data = mainOutput[mainOutput['questionAndAnswerNickname'] == question_labels[qq]]\n",
    "                \n",
    "                question_correct_bool[qq] = (qq_data['questionAndAnswerCorrectAnswer'].item() == qq_data['questionAndAnswerResponse'].item())\n",
    "                \n",
    "            percent_correct = sum(question_correct_bool) / num_questions * 100 \n",
    "            \n",
    "            # calculate reading speed\n",
    "            speed_data = mainOutput[mainOutput['conditionName'] == condition_name]\n",
    "            numWords = speed_data['readingPageWords'].dropna()\n",
    "            reading_time = speed_data['readingPageDurationOnsetToOffsetSec'].dropna()\n",
    "\n",
    "            numWords_include = numWords[1:len(numWords)-1]  # exclude first and last page\n",
    "            reading_time_include = reading_time[1:len(reading_time)-1]\n",
    "\n",
    "            readingSpeed = numWords_include / (reading_time_include / 60)\n",
    "\n",
    "            if readingSpeed.empty:\n",
    "                print(f'Warning: no reading speed data, session name: {files[sess]}, condition: {condition_name}')\n",
    "\n",
    "            if log_bool:\n",
    "                readingSpeed = np.log10(readingSpeed)\n",
    "\n",
    "            for pg in range(len(readingSpeed)):\n",
    "\n",
    "                cond_wpm['prolificID'] = prolificID\n",
    "                cond_wpm['conditionName'] = condition_name\n",
    "                cond_wpm['taskName'] = taskName\n",
    "                cond_wpm['meridian'] = meridian\n",
    "                cond_wpm['repeat'] = repeat\n",
    "                cond_wpm['trial_index'] = pg + 1\n",
    "                cond_wpm['threshold'] = readingSpeed.iloc[pg]\n",
    "                cond_wpm['numTrialsSent'] = np.nan\n",
    "                # cond_wpm['questSD'] = np.nan\n",
    "                cond_wpm['readingCQAccuracy'] = percent_correct\n",
    "\n",
    "                all_data.append(cond_wpm)\n",
    "        \n",
    "        all_data_df = pd.DataFrame(all_data)\n",
    "        \n",
    "    return all_data_df\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "256da66a",
   "metadata": {},
   "source": [
    "### Acquire thresholds:\n",
    "\n",
    "- letter acuity (log deg)\n",
    "- crowding acuity (log deg)\n",
    "- RSVP reading speed (word duration, log sec)\n",
    "- ordinary reading speed (words per min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f06e24fe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fatal: no trials were sent to quest, session name: FreeSilverFish342_67bdf5a74b3f256db8907428_CrowdingReadingAcuity_firstSess20_0001_2025-07-02_19h51.21.712_EDT.csv; condition: crowding_R8_block1\n",
      "Fatal: no trials were sent to quest, session name: FreeSilverFish342_67bdf5a74b3f256db8907428_CrowdingReadingAcuity_firstSess20_0001_2025-07-02_19h51.21.712_EDT.csv; condition: crowding_L8_block1\n",
      "Fatal: no trials were sent to quest, session name: FreeSilverFish342_67bdf5a74b3f256db8907428_CrowdingReadingAcuity_firstSess20_0001_2025-07-02_19h51.21.712_EDT.csv; condition: crowding_R8_block2\n",
      "Fatal: no trials were sent to quest, session name: FreeSilverFish342_67bdf5a74b3f256db8907428_CrowdingReadingAcuity_firstSess20_0001_2025-07-02_19h51.21.712_EDT.csv; condition: crowding_L8_block2\n",
      "Fatal: no trials were sent to quest, session name: FreeSilverFish342_67bdf5a74b3f256db8907428_CrowdingReadingAcuity_firstSess20_0001_2025-07-02_19h51.21.712_EDT.csv; condition: acuity_R8_block1\n",
      "Fatal: no trials were sent to quest, session name: FreeSilverFish342_67bdf5a74b3f256db8907428_CrowdingReadingAcuity_firstSess20_0001_2025-07-02_19h51.21.712_EDT.csv; condition: acuity_L8_block1\n"
     ]
    }
   ],
   "source": [
    "# first session\n",
    "\n",
    "thresholds_names_sess1 = ['crowding_R8_block1','crowding_L8_block1',\n",
    "                          'crowding_R8_block2','crowding_L8_block2',\n",
    "                          'acuity_R8_block1','acuity_L8_block1']\n",
    "df_firstSess = get_trialwise_thresholds(dir_firstSess, files_firstSess, thresholds_names_sess1)\n",
    "\n",
    "thresholds_rsvp_sess1 = ['rsvp_foveal_block1']\n",
    "df_firstSess_rsvp = get_trialwise_thresholds(dir_firstSess, files_firstSess, thresholds_rsvp_sess1, convert_to_wpm_bool=True)\n",
    "\n",
    "thresholds_names_read1 = {\n",
    "        'reading_Beaver_block1': ['Beaver_1','Beaver_2','Beaver_3','Beaver_4','Beaver_5'],\n",
    "        'reading_Winter_block2': ['Winter_1','Winter_2','Winter_3','Winter_4','Winter_5']}\n",
    "df_firstSess_reading = getOrdReadingSpeed_pagewise(dir_firstSess, files_firstSess, thresholds_names_read1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c1b68c63",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fatal: no trials were sent to quest, session name: UpBronzeWhale573_5e38397719e44f0244389781_CrowdingReadingAcuity_secondSess13_0001_2025-07-01_17h48.33.278_ADT.csv; condition: crowding_R8_block3\n",
      "Fatal: no trials were sent to quest, session name: UpBronzeWhale573_5e38397719e44f0244389781_CrowdingReadingAcuity_secondSess13_0001_2025-07-01_17h48.33.278_ADT.csv; condition: crowding_L8_block3\n",
      "Fatal: no trials were sent to quest, session name: UpBronzeWhale573_5e38397719e44f0244389781_CrowdingReadingAcuity_secondSess13_0001_2025-07-01_17h48.33.278_ADT.csv; condition: crowding_R8_block4\n",
      "Fatal: no trials were sent to quest, session name: UpBronzeWhale573_5e38397719e44f0244389781_CrowdingReadingAcuity_secondSess13_0001_2025-07-01_17h48.33.278_ADT.csv; condition: crowding_L8_block4\n",
      "Fatal: no trials were sent to quest, session name: UpBronzeWhale573_5e38397719e44f0244389781_CrowdingReadingAcuity_secondSess13_0001_2025-07-01_17h48.33.278_ADT.csv; condition: acuity_R8_block2\n",
      "Fatal: no trials were sent to quest, session name: UpBronzeWhale573_5e38397719e44f0244389781_CrowdingReadingAcuity_secondSess13_0001_2025-07-01_17h48.33.278_ADT.csv; condition: acuity_L8_block2\n"
     ]
    }
   ],
   "source": [
    "# second session\n",
    "\n",
    "thresholds_names_sess2 = ['crowding_R8_block3','crowding_L8_block3',\n",
    "                          'crowding_R8_block4','crowding_L8_block4',\n",
    "                          'acuity_R8_block2','acuity_L8_block2']\n",
    "df_secondSess = get_trialwise_thresholds(dir_secondSess, files_secondSess, thresholds_names_sess2)\n",
    "\n",
    "thresholds_rsvp_sess2 = ['rsvp_foveal_block2']\n",
    "df_secondSess_rsvp = get_trialwise_thresholds(dir_secondSess, files_secondSess, thresholds_rsvp_sess2, convert_to_wpm_bool=True)\n",
    "\n",
    "thresholds_names_read2 = {\n",
    "        'reading_Desert_block1': ['Desert_1','Desert_2','Desert_3','Desert_4','Desert_5'],\n",
    "        'reading_Islands_block2': ['Islands_1','Islands_2','Islands_3','Islands_4','Islands_5']}\n",
    "df_secondSess_reading = getOrdReadingSpeed_pagewise(dir_secondSess, files_secondSess, thresholds_names_read2, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4a6d95c1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check if you wanted to use logged thresholds.\n"
     ]
    }
   ],
   "source": [
    "# merge data frames\n",
    "\n",
    "df_all_sessions = pd.concat([df_firstSess, df_firstSess_rsvp, df_firstSess_reading,\n",
    "                              df_secondSess, df_secondSess_rsvp, df_secondSess_reading], ignore_index=True)\n",
    "\n",
    "# Check if there are any 0 or negative values in df_both_sessions\n",
    "if (df_all_sessions['threshold'] <= 0).any().any():\n",
    "    print(\"Check if you wanted to use logged thresholds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2d049cde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of participants with both sessions: 168\n"
     ]
    }
   ],
   "source": [
    "# only keep participants who completed both sessions\n",
    "df_both_sessions = df_all_sessions.groupby(\"prolificID\").filter(lambda g: set(g[\"repeat\"]) == {1, 2, 3, 4})\n",
    "\n",
    "# num_thresholds_per_subj = 18\n",
    "# assert df_both_sessions[\"prolificID\"].nunique() == len(df_both_sessions) / num_thresholds_per_subj, 'Fatal: Number of thresholds per subject does not match'\n",
    "\n",
    "print(f'\\nNumber of participants with both sessions: {df_both_sessions[\"prolificID\"].nunique()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "951eae92",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_both_sessions_log = df_both_sessions.copy()\n",
    "# if not already log-transformed, do the log transform here\n",
    "# df_both_sessions_log['threshold'] = np.log10(df_both_sessions_log['threshold'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "bf013972",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to csv\n",
    "output_path = 'tidy_both_sessions_thresholds_per_trial_log.csv'\n",
    "df_both_sessions_log.to_csv(output_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a0d82b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
