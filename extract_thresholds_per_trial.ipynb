{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "266e29fa",
   "metadata": {},
   "source": [
    "This notebook extracts thresholds predicted by quest after every trial. \n",
    "It saves the results in a long form (each row is a threshold), named \"tidy_both_sessions_per_trial_log.csv\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1d44a580-e398-466d-a9b2-17e3abc9db91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install seaborn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "13f644fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import seaborn as sns\n",
    "import scipy.stats as stats\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ddaa1d94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of total sessions:  360\n",
      "Number of subjects who completed the second session:  171\n"
     ]
    }
   ],
   "source": [
    "# Set up files\n",
    "dir_firstSess = 'data/corr_firstSess'\n",
    "files_firstSess = [f for f in os.listdir(dir_firstSess) if f.endswith('.csv')]\n",
    "\n",
    "dir_secondSess = 'data/corr_secondSess'\n",
    "files_secondSess = [f for f in os.listdir(dir_secondSess) if f.endswith('.csv')]\n",
    "\n",
    "numTotalSessions = len(files_firstSess) + len(files_secondSess)\n",
    "\n",
    "print('Number of total sessions: ', numTotalSessions)\n",
    "print('Number of subjects who completed the second session: ', len(files_secondSess))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "210b9b01",
   "metadata": {},
   "source": [
    "## Acquire thresholds for all tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5628276",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8b006083",
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkIfComplete(mainOutput):\n",
    "    \n",
    "    complete_val = mainOutput['experimentCompleteBool'].dropna().iloc[0]\n",
    "    complete_bool = str(complete_val) == 'True'\n",
    "    if not complete_bool:\n",
    "        prolificID = mainOutput['ProlificParticipantID'].dropna().iloc[0]\n",
    "    assert complete_bool, 'Fatal: experiment not complete!'                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "96846bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_condition(condition_str):\n",
    "    \"\"\"\n",
    "    Parse a condition string of the form <task>_<meridian>_block<repeat>\n",
    "    Example: 'acuity_R8_block1' -> ('acuity', 'R8', 1)\n",
    "    \"\"\"\n",
    "    match = re.match(r\"^(.*?)_(.*?)_block(\\d+)$\", condition_str)\n",
    "    if match:\n",
    "        task = match.group(1)\n",
    "        meridian = match.group(2)\n",
    "        repeat = int(match.group(3))\n",
    "        return task, meridian, repeat\n",
    "    else:\n",
    "        raise ValueError(f\"String '{condition_str}' is not in the expected format.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "35d9aaca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_trialwise_thresholds(\n",
    "    mydir, files, condition_names,\n",
    "    value_col=\"questMeanBeforeThisTrialResponse\",\n",
    "    linear_scale_bool=False,\n",
    "    convert_to_wpm_bool=False\n",
    "):\n",
    "    \"\"\"\n",
    "    Return one row per QUEST trial with the running threshold value\n",
    "    taken directly from `value_col` (default: 'questMeanBeforeThisTrialResponse').\n",
    "\n",
    "    Columns returned:\n",
    "      prolificID, file, conditionName, taskName, meridian, repeat,\n",
    "      trial_index (1-based within staircase), threshold_value,\n",
    "      numTrialsSent_total (per condition, optional), questSD_end (optional)\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "\n",
    "    for fname in files:\n",
    "        file_path = os.path.join(mydir, fname)\n",
    "        main = pd.read_csv(file_path)\n",
    "        checkIfComplete(main)\n",
    "\n",
    "        prolificID = (main['ProlificParticipantID'].dropna().iloc[0]\n",
    "                      if 'ProlificParticipantID' in main else np.nan)\n",
    "        assert prolificID, f'Fatal: no prolific ID, session name: {fname}'\n",
    "\n",
    "        for condition_name in condition_names:\n",
    "            # subset to condition\n",
    "            cond = main[main['conditionName'] == condition_name].copy()\n",
    "            assert not cond.empty,f'Fatal: empty condition, session name: {fname}'\n",
    "\n",
    "            # only trials sent to QUEST\n",
    "            if 'trialGivenToQuest' not in cond.columns:\n",
    "                # nothing to extract for non-QUEST tasks\n",
    "                print(f'Warning: no column named trialGivenToQuest, session name: {fname}')\n",
    "\n",
    "            cond = cond[cond['trialGivenToQuest'] == True].copy()\n",
    "            if cond.empty:\n",
    "                print(f'Fatal: no trials were sent to quest, session name: {fname}; condition: {condition_name}')\n",
    "\n",
    "\n",
    "            # must have the value column\n",
    "            assert value_col in cond.columns, f'Fatal: no value column {value_col}, session name: {fname}'\n",
    "\n",
    "            # parse condition into parts (your helper)\n",
    "            taskName, meridian, repeat = parse_condition(condition_name)\n",
    "\n",
    "            # numeric series; if set errors to 'coerse', will keep NaNs to preserve alignment if any\n",
    "            vals = pd.to_numeric(cond[value_col], errors='raise')\n",
    "\n",
    "            # optional unit conversions (apply in log domain first)\n",
    "            if convert_to_wpm_bool:\n",
    "                # log10(sec/word) -> log10(words/min)\n",
    "                vals = np.log10(60) - vals\n",
    "\n",
    "            if linear_scale_bool:\n",
    "                vals = 10 ** vals\n",
    "\n",
    "            # optional per-condition metadata\n",
    "            numTrialsSent_total = int((main.loc[main['conditionName'] == condition_name, 'trialGivenToQuest'] == True).sum())\n",
    "            # questSD_end = (pd.to_numeric(cond.get('questSDAtEndOfTrialsLoop', pd.Series(dtype=float)), errors='coerce')\n",
    "            #                .dropna()\n",
    "            #                .iloc[0] if 'questSDAtEndOfTrialsLoop' in cond and cond['questSDAtEndOfTrialsLoop'].notna().any() else np.nan)\n",
    "\n",
    "            # emit rows\n",
    "            for t in range(len(vals)):\n",
    "                rows.append({\n",
    "                    'prolificID': prolificID,\n",
    "                    'conditionName': condition_name,\n",
    "                    'taskName': taskName,\n",
    "                    'meridian': meridian,\n",
    "                    'repeat': repeat,\n",
    "                    'trial_index': t + 1,            # 1-based\n",
    "                    'threshold': vals.iloc[t],  # taken directly from value_col\n",
    "                    'numTrialsSent': numTrialsSent_total,\n",
    "                    'readingCQAccuracy': np.nan\n",
    "                    # 'questSD_end': questSD_end,\n",
    "                })\n",
    "\n",
    "    return pd.DataFrame(rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7eb47d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getOrdReadingSpeed_pagewise(mydir, files, condition_dict, sessionName, log_bool = True):\n",
    "\t'''\n",
    "\taccuracy_criterion_percent: the reading speed will be marked as np.nan if the accuracy for the comprehension question\n",
    "\t\t\t\t\t\t\t\tis lower than this percentage\n",
    "\twpm_criteiron: the reading speed will be marked as np.nan if it is higher than this percentage\n",
    "\t'''\n",
    "\n",
    "\tcondition_names = list(condition_dict.keys())\n",
    "\n",
    "\tall_data = []\n",
    "\n",
    "\tnumSess = len(files)\n",
    "\tnumThresholdsCat = len(condition_names)\n",
    "\n",
    "\tfor sess in range(numSess):\n",
    "\t\t\n",
    "\t\t# Read the CSV file\n",
    "\t\tfile_path = os.path.join(mydir, files[sess])\n",
    "\t\tmainOutput = pd.read_csv(file_path)\n",
    "\t\tcheckIfComplete(mainOutput)    \n",
    "\t\t\n",
    "\t\tfor cat in range(numThresholdsCat):\n",
    "\n",
    "\t\t\t\n",
    "\t\t\t\t\n",
    "\t\t\tprolificID = mainOutput['ProlificParticipantID'].dropna().iloc[0]\n",
    "\t\t\t\n",
    "\t\t\twordsPerMin = 0\n",
    "\t\t\tcondition_name = condition_names[cat]\n",
    "\t\t\ttaskName, meridian, repeat = parse_condition(condition_name)\n",
    "\t\t\trepeat = sessionName # for reading, the repeats in one session are assigned to the same repeat\n",
    "\n",
    "\t\t\t# Check if the participant answered 3 or more questions correctly\n",
    "\t\t\tquestion_labels = condition_dict[condition_name]\n",
    "\t\t\tnum_questions = len(question_labels)\n",
    "\t\t\tquestion_correct_bool = np.full(num_questions,np.nan)\n",
    "\t\t\tfor qq in range(num_questions):\n",
    "\n",
    "\t\t\t\tqq_data = mainOutput[mainOutput['questionAndAnswerNickname'] == question_labels[qq]]\n",
    "\t\t\t\t\n",
    "\t\t\t\tquestion_correct_bool[qq] = (qq_data['questionAndAnswerCorrectAnswer'].item() == qq_data['questionAndAnswerResponse'].item())\n",
    "\t\t\t\t\n",
    "\t\t\tpercent_correct = sum(question_correct_bool) / num_questions * 100 \n",
    "\t\t\t\n",
    "\t\t\t# calculate reading speed\n",
    "\t\t\tspeed_data = mainOutput[mainOutput['conditionName'] == condition_name]\n",
    "\t\t\tnumWords = speed_data['readingPageWords'].dropna()\n",
    "\t\t\treading_time = speed_data['readingPageDurationOnsetToOffsetSec'].dropna()\n",
    "\n",
    "\t\t\tnumWords_include = numWords.iloc[1:len(numWords)-1]  # exclude first and last page\n",
    "\t\t\treading_time_include = reading_time.iloc[1:len(reading_time)-1]\n",
    "\n",
    "\t\t\treadingSpeed_cumulative = []\n",
    "\t\t\tfor page in range(len(numWords_include)):\n",
    "\n",
    "\n",
    "\t\t\t\tnWords = numWords_include.iloc[0:page+1]\n",
    "\t\t\t\treadtime = reading_time_include.iloc[0:page+1]\n",
    "\t\t\t\tassert len(nWords) == len(readtime), 'Fatal: dimensions do not match'\n",
    "\n",
    "\t\t\t\tsum_nwords = nWords.sum()\n",
    "\t\t\t\tsum_readtime = readtime.sum()\n",
    "\n",
    "\t\t\t\treadingSpeed_cumulative.append(sum_nwords / (sum_readtime/60))\n",
    "\n",
    "\n",
    "\n",
    "\t\t\tif len(readingSpeed_cumulative) == 0:\n",
    "\t\t\t\tprint(f'Warning: no reading speed data, session name: {files[sess]}, condition: {condition_name}')\n",
    "\n",
    "\t\t\tif log_bool:\n",
    "\t\t\t\treadingSpeed_cumulative = np.log10(readingSpeed_cumulative)\n",
    "\n",
    "\n",
    "\n",
    "\t\t\tfor pg in range(len(readingSpeed_cumulative)):\n",
    "\n",
    "\t\t\t\tpage_ind = pg + 1 + 1 # starting from the second page, and ind + 1\n",
    "\t\t\t\tcond_wpm = {}\n",
    "\t\t\t\tcond_wpm['prolificID'] = prolificID\n",
    "\t\t\t\tcond_wpm['conditionName'] = condition_name\n",
    "\t\t\t\tcond_wpm['taskName'] = taskName\n",
    "\t\t\t\tcond_wpm['meridian'] = meridian\n",
    "\t\t\t\tcond_wpm['repeat'] = repeat\n",
    "\t\t\t\tcond_wpm['trial_index'] = page_ind\n",
    "\t\t\t\tcond_wpm['threshold'] = readingSpeed_cumulative[pg]\n",
    "\t\t\t\tcond_wpm['numTrialsSent'] = np.nan\n",
    "\t\t\t\t# cond_wpm['questSD'] = np.nan\n",
    "\t\t\t\tcond_wpm['readingCQAccuracy'] = percent_correct\n",
    "\n",
    "\t\t\t\tall_data.append(cond_wpm)\n",
    "\t\t\n",
    "\t\tall_data_df = pd.DataFrame(all_data)\n",
    "\t\t\n",
    "\treturn all_data_df\n",
    "\t\t\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "658623aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getOrdReadingSpeed(mydir, files, condition_dict, sessionName, log_bool = True):\n",
    "    '''\n",
    "    accuracy_criterion_percent: the reading speed will be marked as np.nan if the accuracy for the comprehension question\n",
    "                                is lower than this percentage\n",
    "    wpm_criteiron: the reading speed will be marked as np.nan if it is higher than this percentage\n",
    "    '''\n",
    "    \n",
    "    condition_names = list(condition_dict.keys())\n",
    "    \n",
    "    all_data = []\n",
    "    \n",
    "    numSess = len(files)\n",
    "    numThresholdsCat = len(condition_names)\n",
    "    \n",
    "    for sess in range(numSess):\n",
    "        \n",
    "        # Read the CSV file\n",
    "        file_path = os.path.join(mydir, files[sess])\n",
    "        mainOutput = pd.read_csv(file_path)\n",
    "        checkIfComplete(mainOutput)    \n",
    "        \n",
    "        for cat in range(numThresholdsCat):\n",
    "\n",
    "            cond_wpm = {}\n",
    "                \n",
    "            prolificID = mainOutput['ProlificParticipantID'].dropna().iloc[0]\n",
    "            \n",
    "            wordsPerMin = 0\n",
    "            condition_name = condition_names[cat]\n",
    "            taskName, meridian, repeat = parse_condition(condition_name)\n",
    "            repeat = sessionName # for reading, the repeats in one session are assigned to the same repeat\n",
    "\n",
    "            # Check if the participant answered 3 or more questions correctly\n",
    "            question_labels = condition_dict[condition_name]\n",
    "            num_questions = len(question_labels)\n",
    "            question_correct_bool = np.full(num_questions,np.nan)\n",
    "            for qq in range(num_questions):\n",
    "\n",
    "                qq_data = mainOutput[mainOutput['questionAndAnswerNickname'] == question_labels[qq]]\n",
    "                \n",
    "                question_correct_bool[qq] = (qq_data['questionAndAnswerCorrectAnswer'].item() == qq_data['questionAndAnswerResponse'].item())\n",
    "                \n",
    "            percent_correct = sum(question_correct_bool) / num_questions * 100 \n",
    "            \n",
    "            # calculate reading speed\n",
    "            speed_data = mainOutput[mainOutput['conditionName'] == condition_name]\n",
    "            numWords = speed_data['readingPageWords'].dropna()\n",
    "            reading_time = speed_data['readingPageDurationOnsetToOffsetSec'].dropna()\n",
    "\n",
    "            numWords_include = numWords[1:len(numWords)-1]  # exclude first and last page\n",
    "            reading_time_include = reading_time[1:len(reading_time)-1]\n",
    "\n",
    "            numWords_sum = numWords_include.sum()\n",
    "            reading_time_sum = reading_time_include.sum()\n",
    "\n",
    "            wordsPerMin = numWords_sum / (reading_time_sum / 60)\n",
    "\n",
    "            if log_bool:\n",
    "                wordsPerMin = np.log10(wordsPerMin)\n",
    "\n",
    "            assert wordsPerMin != 0, 'Fatal: Threshold not assigned'\n",
    "\n",
    "            cond_wpm['prolificID'] = prolificID\n",
    "            cond_wpm['conditionName'] = condition_name\n",
    "            cond_wpm['taskName'] = taskName\n",
    "            cond_wpm['meridian'] = meridian\n",
    "            cond_wpm['repeat'] = repeat\n",
    "            cond_wpm['trial_index'] = meridian\n",
    "            cond_wpm['threshold'] = wordsPerMin\n",
    "            cond_wpm['numTrialsSent'] = np.nan\n",
    "            cond_wpm['questSD'] = np.nan\n",
    "            cond_wpm['readingCQAccuracy'] = percent_correct\n",
    "\n",
    "            all_data.append(cond_wpm)\n",
    "        \n",
    "        all_data_df = pd.DataFrame(all_data)\n",
    "        \n",
    "    return all_data_df\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "256da66a",
   "metadata": {},
   "source": [
    "### Acquire thresholds, trial wise:\n",
    "\n",
    "- letter acuity (log deg)\n",
    "- crowding acuity (log deg)\n",
    "- RSVP reading speed (word duration, log sec)\n",
    "- ordinary reading speed (words per min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f06e24fe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fatal: no trials were sent to quest, session name: FreeSilverFish342_67bdf5a74b3f256db8907428_CrowdingReadingAcuity_firstSess20_0001_2025-07-02_19h51.21.712_EDT.csv; condition: crowding_R8_block1\n",
      "Fatal: no trials were sent to quest, session name: FreeSilverFish342_67bdf5a74b3f256db8907428_CrowdingReadingAcuity_firstSess20_0001_2025-07-02_19h51.21.712_EDT.csv; condition: crowding_L8_block1\n",
      "Fatal: no trials were sent to quest, session name: FreeSilverFish342_67bdf5a74b3f256db8907428_CrowdingReadingAcuity_firstSess20_0001_2025-07-02_19h51.21.712_EDT.csv; condition: crowding_R8_block2\n",
      "Fatal: no trials were sent to quest, session name: FreeSilverFish342_67bdf5a74b3f256db8907428_CrowdingReadingAcuity_firstSess20_0001_2025-07-02_19h51.21.712_EDT.csv; condition: crowding_L8_block2\n",
      "Fatal: no trials were sent to quest, session name: FreeSilverFish342_67bdf5a74b3f256db8907428_CrowdingReadingAcuity_firstSess20_0001_2025-07-02_19h51.21.712_EDT.csv; condition: acuity_R8_block1\n",
      "Fatal: no trials were sent to quest, session name: FreeSilverFish342_67bdf5a74b3f256db8907428_CrowdingReadingAcuity_firstSess20_0001_2025-07-02_19h51.21.712_EDT.csv; condition: acuity_L8_block1\n",
      "Fatal: no trials were sent to quest, session name: UpBronzeWhale573_5e38397719e44f0244389781_CrowdingReadingAcuity_secondSess13_0001_2025-07-01_17h48.33.278_ADT.csv; condition: crowding_R8_block3\n",
      "Fatal: no trials were sent to quest, session name: UpBronzeWhale573_5e38397719e44f0244389781_CrowdingReadingAcuity_secondSess13_0001_2025-07-01_17h48.33.278_ADT.csv; condition: crowding_L8_block3\n",
      "Fatal: no trials were sent to quest, session name: UpBronzeWhale573_5e38397719e44f0244389781_CrowdingReadingAcuity_secondSess13_0001_2025-07-01_17h48.33.278_ADT.csv; condition: crowding_R8_block4\n",
      "Fatal: no trials were sent to quest, session name: UpBronzeWhale573_5e38397719e44f0244389781_CrowdingReadingAcuity_secondSess13_0001_2025-07-01_17h48.33.278_ADT.csv; condition: crowding_L8_block4\n",
      "Fatal: no trials were sent to quest, session name: UpBronzeWhale573_5e38397719e44f0244389781_CrowdingReadingAcuity_secondSess13_0001_2025-07-01_17h48.33.278_ADT.csv; condition: acuity_R8_block2\n",
      "Fatal: no trials were sent to quest, session name: UpBronzeWhale573_5e38397719e44f0244389781_CrowdingReadingAcuity_secondSess13_0001_2025-07-01_17h48.33.278_ADT.csv; condition: acuity_L8_block2\n"
     ]
    }
   ],
   "source": [
    "# first session\n",
    "\n",
    "thresholds_names_sess1 = ['crowding_R8_block1','crowding_L8_block1',\n",
    "                          'crowding_R8_block2','crowding_L8_block2',\n",
    "                          'acuity_R8_block1','acuity_L8_block1']\n",
    "df_firstSess = get_trialwise_thresholds(dir_firstSess, files_firstSess, thresholds_names_sess1)\n",
    "\n",
    "thresholds_rsvp_sess1 = ['rsvp_foveal_block1']\n",
    "df_firstSess_rsvp = get_trialwise_thresholds(dir_firstSess, files_firstSess, thresholds_rsvp_sess1, convert_to_wpm_bool=True)\n",
    "\n",
    "thresholds_names_read1 = {\n",
    "        'reading_Beaver_block1': ['Beaver_1','Beaver_2','Beaver_3','Beaver_4','Beaver_5'],\n",
    "        'reading_Winter_block2': ['Winter_1','Winter_2','Winter_3','Winter_4','Winter_5']}\n",
    "df_firstSess_reading = getOrdReadingSpeed(dir_firstSess, files_firstSess, thresholds_names_read1, 1)\n",
    "\n",
    "# second session\n",
    "\n",
    "thresholds_names_sess2 = ['crowding_R8_block3','crowding_L8_block3',\n",
    "                          'crowding_R8_block4','crowding_L8_block4',\n",
    "                          'acuity_R8_block2','acuity_L8_block2']\n",
    "df_secondSess = get_trialwise_thresholds(dir_secondSess, files_secondSess, thresholds_names_sess2)\n",
    "\n",
    "thresholds_rsvp_sess2 = ['rsvp_foveal_block2']\n",
    "df_secondSess_rsvp = get_trialwise_thresholds(dir_secondSess, files_secondSess, thresholds_rsvp_sess2, convert_to_wpm_bool=True)\n",
    "\n",
    "thresholds_names_read2 = {\n",
    "        'reading_Desert_block1': ['Desert_1','Desert_2','Desert_3','Desert_4','Desert_5'],\n",
    "        'reading_Islands_block2': ['Islands_1','Islands_2','Islands_3','Islands_4','Islands_5']}\n",
    "df_secondSess_reading = getOrdReadingSpeed(dir_secondSess, files_secondSess, thresholds_names_read2, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4a6d95c1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check if you wanted to use logged thresholds.\n"
     ]
    }
   ],
   "source": [
    "# merge data frames\n",
    "\n",
    "df_all_sessions = pd.concat([df_firstSess, df_firstSess_rsvp, df_firstSess_reading,\n",
    "                              df_secondSess, df_secondSess_rsvp, df_secondSess_reading], ignore_index=True)\n",
    "\n",
    "# Check if there are any 0 or negative values in df_both_sessions\n",
    "if (df_all_sessions['threshold'] <= 0).any().any():\n",
    "    print(\"Check if you wanted to use logged thresholds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2d049cde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of participants with both sessions: 168\n"
     ]
    }
   ],
   "source": [
    "# only keep participants who completed both sessions\n",
    "df_both_sessions = df_all_sessions.groupby(\"prolificID\").filter(lambda g: set(g[\"repeat\"]) == {1, 2, 3, 4})\n",
    "\n",
    "# num_thresholds_per_subj = 18\n",
    "# assert df_both_sessions[\"prolificID\"].nunique() == len(df_both_sessions) / num_thresholds_per_subj, 'Fatal: Number of thresholds per subject does not match'\n",
    "\n",
    "print(f'\\nNumber of participants with both sessions: {df_both_sessions[\"prolificID\"].nunique()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "951eae92",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_both_sessions_log = df_both_sessions.copy()\n",
    "# if not already log-transformed, do the log transform here\n",
    "# df_both_sessions_log['threshold'] = np.log10(df_both_sessions_log['threshold'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1e506698",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['prolificID', 'conditionName', 'taskName', 'meridian', 'repeat',\n",
      "       'trial_index', 'threshold', 'numTrialsSent', 'readingCQAccuracy',\n",
      "       'questSD'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df_both_sessions_log.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "14f807f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# updated trial number to match passage number\n",
    "df_both_sessions_log.loc[\n",
    "    (df_both_sessions_log['taskName'] == 'reading') & \n",
    "    (df_both_sessions_log['meridian'].isin(['Beaver'])),\n",
    "    'trial_index'\n",
    "] = 1\n",
    "\n",
    "df_both_sessions_log.loc[\n",
    "    (df_both_sessions_log['taskName'] == 'reading') & \n",
    "    (df_both_sessions_log['meridian'].isin(['Winter'])),\n",
    "    'trial_index'\n",
    "] = 2\n",
    "\n",
    "df_both_sessions_log.loc[\n",
    "    (df_both_sessions_log['taskName'] == 'reading') & \n",
    "    (df_both_sessions_log['meridian'].isin(['Desert'])),\n",
    "    'trial_index'\n",
    "] = 1\n",
    "\n",
    "df_both_sessions_log.loc[\n",
    "    (df_both_sessions_log['taskName'] == 'reading') & \n",
    "    (df_both_sessions_log['meridian'].isin(['Islands'])),\n",
    "    'trial_index'\n",
    "] = 2\n",
    "\n",
    "\n",
    "# Update meridian values for reading tasks\n",
    "df_both_sessions_log.loc[\n",
    "    (df_both_sessions_log['taskName'] == 'reading') & \n",
    "    (df_both_sessions_log['meridian'].isin(['Beaver', 'Desert'])),\n",
    "    'meridian'\n",
    "] = 'first'\n",
    "\n",
    "df_both_sessions_log.loc[\n",
    "    (df_both_sessions_log['taskName'] == 'reading') & \n",
    "    (df_both_sessions_log['meridian'].isin(['Winter', 'Islands'])),\n",
    "    'meridian'\n",
    "] = 'second'\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "457548ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44997    1\n",
       "44998    2\n",
       "44999    1\n",
       "45000    2\n",
       "45001    1\n",
       "        ..\n",
       "86845    2\n",
       "86846    1\n",
       "86847    2\n",
       "86848    1\n",
       "86849    2\n",
       "Name: trial_index, Length: 672, dtype: object"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_both_sessions_log.loc[\n",
    "    (df_both_sessions_log['taskName'] == 'reading'),\n",
    "    'trial_index'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "cbd48ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_reading = (df_both_sessions_log['taskName'] == 'reading')\n",
    "\n",
    "# compute mean across trial_index (1 & 2) within each (prolificID, repeat)\n",
    "# (does NOT average across repeats)\n",
    "df_both_sessions_log.loc[mask_reading, 'reading_mean'] = (\n",
    "    df_both_sessions_log.loc[mask_reading]\n",
    "      .groupby(['prolificID', 'repeat'])['threshold']\n",
    "      .transform('mean')\n",
    ")\n",
    "\n",
    "# replace threshold only for trial_index == 2\n",
    "mask_update = mask_reading & (df_both_sessions_log['trial_index'] == 2)\n",
    "df_both_sessions_log.loc[mask_update, 'threshold'] = df_both_sessions_log.loc[mask_update, 'reading_mean']\n",
    "\n",
    "# optional: clean up helper column\n",
    "df_both_sessions_log.drop(columns='reading_mean', inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "bf013972",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to csv\n",
    "output_path = 'tidy_both_sessions_thresholds_per_trial_log.csv'\n",
    "df_both_sessions_log.to_csv(output_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb7b6b7b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
