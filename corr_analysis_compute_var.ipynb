{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d44a580-e398-466d-a9b2-17e3abc9db91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install seaborn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "13f644fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import seaborn as sns\n",
    "import scipy.stats as stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ddaa1d94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of total sessions:  316\n"
     ]
    }
   ],
   "source": [
    "# Set up files\n",
    "dir_firstSess = 'data/corr_firstSess'\n",
    "files_firstSess = [f for f in os.listdir(dir_firstSess) if f.endswith('.csv')]\n",
    "\n",
    "dir_secondSess = 'data/corr_secondSess'\n",
    "files_secondSess = [f for f in os.listdir(dir_secondSess) if f.endswith('.csv')]\n",
    "\n",
    "numTotalSessions = len(files_firstSess) + len(files_secondSess)\n",
    "\n",
    "print('Number of total sessions: ', numTotalSessions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bddc953",
   "metadata": {},
   "source": [
    "# Data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a8d18c0",
   "metadata": {},
   "source": [
    "## Check if all experiments are completed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3cbf14fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkIfComplete(mydir, files):\n",
    "    \n",
    "    numSess = len(files)\n",
    "    \n",
    "    for sess in range(numSess):\n",
    "\n",
    "        # Read the CSV file\n",
    "        file_path = os.path.join(mydir, files[sess])\n",
    "        mainOutput = pd.read_csv(file_path)\n",
    "\n",
    "        # Exclusion criteria 1: experiment completed\n",
    "        # print(files[sess])\n",
    "        complete_val = mainOutput['experimentCompleteBool'].dropna().iloc[0]\n",
    "        complete_bool = str(complete_val) == 'True'\n",
    "        if not complete_bool:\n",
    "            prolificID = mainOutput['ProlificParticipantID'].dropna().iloc[0]\n",
    "            print(f'Warning: incomplete experiment (session: {sess})')\n",
    "            \n",
    "    print(f'Completion check completed! ({numSess} files total)')\n",
    "                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a993b63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion check completed! (167 files total)\n"
     ]
    }
   ],
   "source": [
    "checkIfComplete(dir_firstSess, files_firstSess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9e84f46d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion check completed! (149 files total)\n"
     ]
    }
   ],
   "source": [
    "checkIfComplete(dir_secondSess, files_secondSess)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "210b9b01",
   "metadata": {},
   "source": [
    "## Acquire thresholds for all tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5628276",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "31cb1596",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getThresholds(mydir, files, condition_names, num_trials_per_staircase=35,\n",
    "                     exclude_trial_count_bool=True, exclude_questSD=True):\n",
    "    \n",
    "    all_data = []\n",
    "    \n",
    "    numSess = len(files)\n",
    "    numThresholdsCat = len(condition_names)\n",
    "    \n",
    "    for sess in range(numSess):\n",
    "\n",
    "        # Read the CSV file\n",
    "        file_path = os.path.join(mydir, files[sess])\n",
    "        mainOutput = pd.read_csv(file_path)\n",
    "        subj_logThresholds = {}\n",
    "\n",
    "        # print(files[sess])\n",
    "        # print(mainOutput['ProlificParticipantID'].dropna())\n",
    "        prolificID = mainOutput['ProlificParticipantID'].dropna().iloc[0]\n",
    "        subj_logThresholds['prolificID'] = prolificID\n",
    "                \n",
    "        for cat in range(numThresholdsCat):\n",
    "            \n",
    "            condition_name = condition_names[cat]\n",
    "                        \n",
    "            condition_data = mainOutput[mainOutput['conditionName'] == condition_name]\n",
    "\n",
    "            assert(len(condition_data.questMeanAtEndOfTrialsLoop.dropna()) == 1)\n",
    "            subj_logThresholds[condition_name] = condition_data.questMeanAtEndOfTrialsLoop.dropna().iloc[0]\n",
    "\n",
    "            if exclude_trial_count_bool: \n",
    "                # Count trials sent to quest\n",
    "                trial_sent = condition_data['trialGivenToQuest']           \n",
    "                num_trial_sent = sum(str(this_trial) == 'True' for this_trial in trial_sent)\n",
    "                num_trial_not_sent = sum(str(this_trial) == 'False' for this_trial in trial_sent)\n",
    "                trial_sent_bool = num_trial_sent >= num_trials_per_staircase\n",
    "                num_missing_line = sum(trial_sent.isna())\n",
    "                if not trial_sent_bool:\n",
    "                    subj_logThresholds[condition_name] = np.nan\n",
    "                    # print(files[sess])\n",
    "                    # print(f'Warning1: not enough trials (Session {sess}, condition {condition_name})')\n",
    "                    # print(f'Num total trials: {len(trial_sent) - 1}')\n",
    "                    # print(f'Num trials missing: {num_trials_per_staircase - num_trial_sent}')\n",
    "                    # print(f'Num trials marked as not sent: {num_trial_not_sent}')\n",
    "                    # print(f'Num lines missing: {num_missing_line - 1}')\n",
    "\n",
    "            if exclude_questSD:\n",
    "                questSD = condition_data['questSDAtEndOfTrialsLoop'].dropna().iloc[0]\n",
    "                small_questSD_bool = questSD < 0.1\n",
    "                if not small_questSD_bool:\n",
    "                    subj_logThresholds[condition_name] = np.nan\n",
    "                    # print(f'Warning2: large SD (Session {sess}, condition {condition_name}, SD = {questSD})')\n",
    "        \n",
    "        all_data.append(subj_logThresholds)\n",
    "        \n",
    "        all_data_df = pd.DataFrame(all_data)\n",
    "        \n",
    "    return all_data_df\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a37f23df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getRSVPThresholds(mydir, files, condition_names, num_trials_per_staircase=24,\n",
    "                     exclude_trial_count_bool=True, exclude_questSD=True):\n",
    "    \n",
    "    all_data = []\n",
    "    \n",
    "    numSess = len(files)\n",
    "    numThresholdsCat = len(condition_names)\n",
    "    \n",
    "    for sess in range(numSess):\n",
    "\n",
    "        file_path = os.path.join(mydir, files[sess])\n",
    "        mainOutput = pd.read_csv(file_path)\n",
    "        subj_wpm = {}\n",
    "                \n",
    "        prolificID = mainOutput['ProlificParticipantID'].dropna().iloc[0]\n",
    "        subj_wpm['prolificID'] = prolificID\n",
    "                \n",
    "        for cat in range(numThresholdsCat):\n",
    "            \n",
    "            condition_name = condition_names[cat]\n",
    "                        \n",
    "            condition_data = mainOutput[mainOutput['conditionName'] == condition_name]\n",
    "\n",
    "            # Extract threshold: \n",
    "            # check that only 1 threshold is reported for this condition\n",
    "            assert(len(condition_data.questMeanAtEndOfTrialsLoop.dropna()) == 1)\n",
    "            thresholds_raw_log = condition_data.questMeanAtEndOfTrialsLoop.dropna().iloc[0]\n",
    "            subj_wpm[condition_name] = np.power(10, np.log10(60) - thresholds_raw_log)\n",
    "\n",
    "            if exclude_trial_count_bool: \n",
    "                        \n",
    "                # Count trials sent to quest\n",
    "                trial_sent = condition_data['trialGivenToQuest']           \n",
    "                num_trial_sent = sum(str(this_trial) == 'True' for this_trial in trial_sent)\n",
    "                num_trial_not_sent = sum(str(this_trial) == 'False' for this_trial in trial_sent)\n",
    "                trial_sent_bool = num_trial_sent >= num_trials_per_staircase\n",
    "                num_missing_line = sum(trial_sent.isna())\n",
    "\n",
    "                if not trial_sent_bool:\n",
    "                    subj_wpm[condition_name] = np.nan\n",
    "                    # print(files[sess])\n",
    "                    # print(f'Warning1: not enough trials (Session {sess}, condition {condition_name})')\n",
    "                    # print(f'Num total trials: {len(trial_sent) - 1}')\n",
    "                    # print(f'Num trials missing: {num_trials_per_staircase - num_trial_sent}')\n",
    "                    # print(f'Num trials marked as not sent: {num_trial_not_sent}')\n",
    "                    # print(f'Num lines missing: {num_missing_line - 1}')\n",
    "\n",
    "            if exclude_questSD:\n",
    "                questSD = condition_data['questSDAtEndOfTrialsLoop'].dropna().iloc[0]\n",
    "                small_questSD_bool = questSD < 0.1\n",
    "                \n",
    "                if not small_questSD_bool:\n",
    "                    subj_wpm[condition_name] = np.nan\n",
    "                    # print(f'Warning2: large SD (Session {sess}, condition {condition_name}, SD = {questSD})')\n",
    "            \n",
    "        \n",
    "        all_data.append(subj_wpm)\n",
    "        \n",
    "        all_data_df = pd.DataFrame(all_data)\n",
    "        \n",
    "    return all_data_df\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "31027c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getOrdReadingSpeed(mydir, files, condition_dict,\n",
    "                       accuracy_criterion_percent=60, wpm_criterion=400):\n",
    "    '''\n",
    "    accuracy_criterion_percent: the reading speed will be marked as np.nan if the accuracy for the comprehension question\n",
    "                                is lower than this percentage\n",
    "    wpm_criteiron: the reading speed will be marked as np.nan if it is higher than this percentage\n",
    "    '''\n",
    "    \n",
    "    condition_names = list(condition_dict.keys())\n",
    "    \n",
    "    all_data = []\n",
    "    \n",
    "    numSess = len(files)\n",
    "    numThresholdsCat = len(condition_names)\n",
    "    \n",
    "    for sess in range(numSess):\n",
    "        \n",
    "        # Read the CSV file\n",
    "        file_path = os.path.join(mydir, files[sess])\n",
    "        mainOutput = pd.read_csv(file_path)\n",
    "        subj_wpm = {}\n",
    "                \n",
    "        prolificID = mainOutput['ProlificParticipantID'].dropna().iloc[0]\n",
    "        subj_wpm['prolificID'] = prolificID\n",
    "        \n",
    "        for cat in range(numThresholdsCat):\n",
    "            \n",
    "            condition_name = condition_names[cat]\n",
    "\n",
    "            # Check if the participant answered 3 or more questions correctly\n",
    "            question_labels = condition_dict[condition_name]\n",
    "            num_questions = len(question_labels)\n",
    "            question_correct_bool = np.full(num_questions,np.nan)\n",
    "            for qq in range(num_questions):\n",
    "\n",
    "                qq_data = mainOutput[mainOutput['questionAndAnswerNickname'] == question_labels[qq]]\n",
    "                \n",
    "                question_correct_bool[qq] = (qq_data['questionAndAnswerCorrectAnswer'].item() == qq_data['questionAndAnswerResponse'].item())\n",
    "                \n",
    "            percent_correct = sum(question_correct_bool) / num_questions * 100 \n",
    "            \n",
    "            # calculate reading speed\n",
    "            speed_data = mainOutput[mainOutput['conditionName'] == condition_name]\n",
    "            numWords = speed_data['readingPageWords'].dropna()\n",
    "            reading_time = speed_data['readingPageDurationOnsetToOffsetSec'].dropna()\n",
    "            pg_wordsPerMin = numWords / (reading_time / 60)\n",
    "            include_wordsPerMin = pg_wordsPerMin[1:len(pg_wordsPerMin)-1] # exclude first and last page\n",
    "            subj_wpm[condition_name] = np.mean(include_wordsPerMin)\n",
    "            \n",
    "            if percent_correct < accuracy_criterion_percent: \n",
    "                # print(f'Warning: percent correct is too low: session {sess}, passage {cat}')\n",
    "                subj_wpm[condition_name] = np.nan\n",
    "\n",
    "            if np.mean(include_wordsPerMin) > wpm_criterion:\n",
    "                # print(f'Warning: ordinary reading speed is too high: session {sess}, passage {cat}')\n",
    "                # print(f'-- wpm: {np.mean(include_wordsPerMin)}')\n",
    "                subj_wpm[condition_name] = np.nan\n",
    "\n",
    "    \n",
    "                \n",
    "        all_data.append(subj_wpm)\n",
    "        \n",
    "        all_data_df = pd.DataFrame(all_data)\n",
    "        \n",
    "    return all_data_df\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "256da66a",
   "metadata": {},
   "source": [
    "### Acquire thresholds:\n",
    "\n",
    "- Note that data acquired for RSVP and Ordinary reading are in words per min -- not logged\n",
    "\n",
    "- All the other thresholds are logged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f06e24fe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# first session\n",
    "\n",
    "thresholds_names_sess1 = ['crowding_R8_block1','crowding_L8_block1',\n",
    "                          'crowding_R8_block2','crowding_L8_block2',\n",
    "                          'acuity_R8_block1','acuity_L8_block1']\n",
    "df_firstSess = getThresholds(dir_firstSess, files_firstSess, thresholds_names_sess1, exclude_trial_count_bool=True, exclude_questSD=True)\n",
    "\n",
    "thresholds_vernier_sess1 = ['vernier_R8_block1','vernier_L8_block1']\n",
    "df_firstSess_vernier = getThresholds(dir_firstSess, files_firstSess, thresholds_vernier_sess1, exclude_trial_count_bool=True, exclude_questSD=False)\n",
    "\n",
    "thresholds_rsvp_sess1 = ['rsvp_foveal_block1']\n",
    "df_firstSess_rsvp = getRSVPThresholds(dir_firstSess, files_firstSess, thresholds_rsvp_sess1, exclude_trial_count_bool=True, exclude_questSD=True)\n",
    "\n",
    "thresholds_names_read1 = {\n",
    "        'reading_Beaver_block1': ['Beaver_1','Beaver_2','Beaver_3','Beaver_4','Beaver_5'],\n",
    "        'reading_Winter_block2': ['Winter_1','Winter_2','Winter_3','Winter_4','Winter_5']}\n",
    "df_firstSess_reading = getOrdReadingSpeed(dir_firstSess, files_firstSess, thresholds_names_read1, accuracy_criterion_percent=60, wpm_criterion=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c1b68c63",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# second session\n",
    "\n",
    "thresholds_names_sess2 = ['crowding_R8_block3','crowding_L8_block3',\n",
    "                          'crowding_R8_block4','crowding_L8_block4',\n",
    "                          'acuity_R8_block2','acuity_L8_block2']\n",
    "df_secondSess = getThresholds(dir_secondSess, files_secondSess, thresholds_names_sess2, exclude_trial_count_bool=True, exclude_questSD=True)\n",
    "\n",
    "thresholds_vernier_sess2 = ['vernier_R8_block2','vernier_L8_block2']\n",
    "df_secondSess_vernier = getThresholds(dir_secondSess, files_secondSess, thresholds_vernier_sess2, exclude_trial_count_bool=True, exclude_questSD=False)\n",
    "\n",
    "thresholds_rsvp_sess2 = ['rsvp_foveal_block2']\n",
    "df_secondSess_rsvp = getRSVPThresholds(dir_secondSess, files_secondSess, thresholds_rsvp_sess2, exclude_trial_count_bool=True, exclude_questSD=True)\n",
    "\n",
    "thresholds_names_read2 = {\n",
    "        'reading_Desert_block1': ['Desert_1','Desert_2','Desert_3','Desert_4','Desert_5'],\n",
    "        'reading_Islands_block2': ['Islands_1','Islands_2','Islands_3','Islands_4','Islands_5']}\n",
    "\n",
    "df_secondSess_reading = getOrdReadingSpeed(dir_secondSess, files_secondSess, thresholds_names_read2, accuracy_criterion_percent=60, wpm_criterion=400)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4a6d95c1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# merge data frames\n",
    "df_first_merge = pd.merge(df_firstSess, df_firstSess_vernier, on=\"prolificID\", how=\"inner\")\n",
    "df_first_merge = pd.merge(df_first_merge, df_firstSess_rsvp, on=\"prolificID\", how=\"inner\")\n",
    "df_first = pd.merge(df_first_merge, df_firstSess_reading, on=\"prolificID\", how=\"inner\")\n",
    "\n",
    "df_second_merge = pd.merge(df_secondSess, df_secondSess_vernier, on=\"prolificID\", how=\"inner\")\n",
    "df_second_merge = pd.merge(df_second_merge, df_secondSess_rsvp, on=\"prolificID\", how=\"inner\")\n",
    "df_second = pd.merge(df_second_merge, df_secondSess_reading, on=\"prolificID\", how=\"inner\")\n",
    "\n",
    "\n",
    "df_both_sessions = pd.merge(df_first, df_second, on=\"prolificID\", how=\"inner\")\n",
    "# display(df_both_sessions)\n",
    "# print(df_both_sessions.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "facaa50c-67de-461c-8fc0-eff0490df425",
   "metadata": {},
   "source": [
    "### For each task, exclude all thresholds from a participant if one threshold is abnormal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3e09852e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nan_out_task(df, task_prefixes):\n",
    "    df = df.copy()\n",
    "    for prefix in task_prefixes:\n",
    "        task_cols = [col for col in df.columns if col.startswith(prefix)]\n",
    "        # Find rows where any of the task columns is nan\n",
    "        mask = df[task_cols].isna().any(axis=1)\n",
    "        # Set all task columns to nan for those rows\n",
    "        df.loc[mask, task_cols] = np.nan\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20185787",
   "metadata": {},
   "outputs": [],
   "source": [
    "task_prefixes = ['crowding', 'vernier', 'acuity', 'rsvp', 'reading']\n",
    "\n",
    "# Apply to your data frame\n",
    "df_both_sessions_exclude_subj_task = nan_out_task(df_both_sessions, task_prefixes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c7da4f3",
   "metadata": {},
   "source": [
    "### Average left and right thresholds for acuity and crowding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "052a48d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_both_sessions_averagedRL = df_both_sessions_exclude_subj_task.copy()\n",
    "\n",
    "# Average left and right thresholds for acuity, crowding, and vernier in each block\n",
    "blocks = ['block1', 'block2', 'block3', 'block4']\n",
    "tasks = ['crowding', 'acuity', 'vernier']\n",
    "\n",
    "for task in tasks:\n",
    "    for block in blocks:\n",
    "        col_R = f\"{task}_R8_{block}\"\n",
    "        col_L = f\"{task}_L8_{block}\"\n",
    "        avg_col = f\"{task}_avg_{block}\"\n",
    "        if col_R in df_both_sessions_averagedRL.columns and col_L in df_both_sessions_averagedRL.columns:\n",
    "            df_both_sessions_averagedRL[avg_col] = (\n",
    "                df_both_sessions_averagedRL[[col_R, col_L]].mean(axis=1)\n",
    "            )\n",
    "\n",
    "# Now df_both_sessions_averagedRL contains averaged columns like 'crowding_avg_block1', etc.\n",
    "# You can select only the averaged columns and prolificID for your new DataFrame:\n",
    "avg_cols = ['prolificID'] + [f\"{task}_avg_{block}\" \n",
    "                             for task in tasks \n",
    "                             for block in blocks \n",
    "                             if f\"{task}_avg_{block}\" in df_both_sessions_averagedRL.columns]\n",
    "\n",
    "df_averaged_thresholds = df_both_sessions_averagedRL[avg_cols].copy()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "58fe241a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['prolificID', 'crowding_avg_block1', 'crowding_avg_block2',\n",
      "       'crowding_avg_block3', 'crowding_avg_block4', 'acuity_avg_block1',\n",
      "       'acuity_avg_block2', 'vernier_avg_block1', 'vernier_avg_block2',\n",
      "       'rsvp_foveal_block1', 'rsvp_foveal_block2', 'reading_Beaver_block1',\n",
      "       'reading_Winter_block2', 'reading_Desert_block1',\n",
      "       'reading_Islands_block2'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Identify all RSVP and reading columns\n",
    "rsvp_cols = [col for col in df_both_sessions_averagedRL.columns if col.startswith('rsvp')]\n",
    "reading_cols = [col for col in df_both_sessions_averagedRL.columns if col.startswith('reading')]\n",
    "\n",
    "# Combine with the averaged columns and prolificID\n",
    "all_cols = ['prolificID'] + \\\n",
    "           [col for col in df_averaged_thresholds.columns if col != 'prolificID'] + \\\n",
    "           rsvp_cols + reading_cols\n",
    "\n",
    "# Remove duplicates while preserving order\n",
    "from collections import OrderedDict\n",
    "all_cols = list(OrderedDict.fromkeys(all_cols))\n",
    "\n",
    "# Create new DataFrame with all desired columns\n",
    "df_averaged_thresholds_with_rsvp_reading = df_both_sessions_averagedRL[all_cols].copy()\n",
    "\n",
    "print(df_averaged_thresholds_with_rsvp_reading.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b92989e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['prolificID', 'crowding_avg_block1', 'crowding_avg_block2',\n",
      "       'crowding_avg_block3', 'crowding_avg_block4', 'acuity_avg_block1',\n",
      "       'acuity_avg_block2', 'vernier_avg_block1', 'vernier_avg_block2'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df_averaged_thresholds.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e14396ef",
   "metadata": {},
   "source": [
    "# Analysis\n",
    "For df_both_sessions and df_both_sessions_exclude_subj_task:\n",
    "- letter, vernier, crowding are in logged deg: when we take the mean directly, it's taking the geometric mean\n",
    "- rsvp and ordinary reading speeds are in words per minute: remember to log before taking the mean! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9a5fb576",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_for_analysis = df_both_sessions_exclude_subj_task.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38944519",
   "metadata": {},
   "source": [
    "### Test-retest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6bba4027",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_for_analysis['crowding_R8_12'] = (df_for_analysis['crowding_R8_block1'] + \n",
    "                                      df_for_analysis['crowding_R8_block2']) / 2\n",
    "\n",
    "df_for_analysis['crowding_L8_12'] = (df_for_analysis['crowding_L8_block1'] + \n",
    "                                      df_for_analysis['crowding_L8_block2']) / 2\n",
    "\n",
    "df_for_analysis['crowding_R8_34'] = (df_for_analysis['crowding_R8_block3'] + \n",
    "                                      df_for_analysis['crowding_R8_block4']) / 2\n",
    "\n",
    "df_for_analysis['crowding_L8_34'] = (df_for_analysis['crowding_L8_block3'] + \n",
    "                                      df_for_analysis['crowding_L8_block4']) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8a556433",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_for_analysis['ordreading_sess1'] = 10 ** ((np.log10(df_for_analysis['reading_Beaver_block1']) + \n",
    "                                               np.log10(df_for_analysis['reading_Winter_block2'])) / 2)\n",
    "\n",
    "df_for_analysis['ordreading_sess2'] = 10 ** ((np.log10(df_for_analysis['reading_Desert_block1']) + \n",
    "                                               np.log10(df_for_analysis['reading_Islands_block2'])) / 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "75c5a96f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['prolificID', 'crowding_R8_block1', 'crowding_L8_block1',\n",
      "       'crowding_R8_block2', 'crowding_L8_block2', 'acuity_R8_block1',\n",
      "       'acuity_L8_block1', 'vernier_R8_block1', 'vernier_L8_block1',\n",
      "       'rsvp_foveal_block1', 'reading_Beaver_block1', 'reading_Winter_block2',\n",
      "       'crowding_R8_block3', 'crowding_L8_block3', 'crowding_R8_block4',\n",
      "       'crowding_L8_block4', 'acuity_R8_block2', 'acuity_L8_block2',\n",
      "       'vernier_R8_block2', 'vernier_L8_block2', 'rsvp_foveal_block2',\n",
      "       'reading_Desert_block1', 'reading_Islands_block2'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df_for_analysis.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75fd107e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
